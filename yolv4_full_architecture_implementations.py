# -*- coding: utf-8 -*-
"""Yolv4_Full_Architecture_Implementations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10zJMQkNqa4ZircDAd4wTcqJfqr0-BbA9
"""

#Here is the full implementation of yolov4 Architecture Architecture...
import torch
import torch.nn as nn
from torch.nn import functional as F

Architecture=[(3,32,3,1,1),
              (32,64,3,2,1),  #Give the result of this into two layers...
              ["Route",(64,64,1,1,0)],  #Here parallelization occur....
              (64,64,1,1,0),
              ["Residual_Block",64,32,64,1],  #Here shortcut operartion occur...
              {"Skip_Connection":(64,64,1,1,0)},  #Output of this layer give as an input to the route

              (128,64,1,1,0),#After concatenation
              (64,128,3,2,1), #reduction occur ...

              ["Route",(64,64,1,1,0)],
              (64,64,1,1,0),
              ["Residual_block",64,64,64,2],
              {"Skip_Connection":(64,64,1,1,0)}, #skip connection

              (128,128,1,1,0), #after concatentation
              (128,256,3,2,1), #reduction occur..

              ["Route",(128,128,1,1,0)],
              (128,128,1,1,0),
              ["Residual_block",128,128,128,8],
              {"Skip_Connection":(128,128,1,1,0)},

              ("Route_1",256,256,1,1,0), #After concatention
              (256,512,3,2,1), #reduction occur

              ["Route",(256,256,1,1,0)],
              (256,256,1,1,0),
              ["Residual_block",256,256,256,8], #Fourth Residual Block with 8 times iterations
              {"Skip_Connection":(256,256,1,1,0)},

              ("Route_2",512,512,1,1,0),
              (512,1024,3,2,1), #reduction occur

              ["Route",(512,512,1,1,0)],
              (512,512,1,1,0),
              ["Residual_block",512,512,512,4], #Fifth Residual Block with 8 times iteration
              {"Skip_Connection":(512,512,1,1,0)},
              (1024,1024,1,1,0), #The final layer...
              ["Leaky",(1024,512,1,1,0)], #Leaky_function executed
              ["Leaky",(512,1024,3,1,1)],
              ["Leaky",(1024,512,1,1,0)], #after this we can apply max pooling...
              #After this there will be spatials pyramid pooling ..It is directly implemented in spatial_pyramid_pooling class
              ]

FPN_Architecture=[["Leaky",(2048,512,1,1,0)],
                  ["Leaky",(512,1024,3,1,1)],
                  ("FPN_Route_1",1024,512,1,1,0), #Route_1
                  (512,256,1,1,0),
                  #After this upscaling done ..Then from the Backbone..The features concatenated with the with the FPN..
                  (512,256,1,1,0),
                  (256,512,3,1,1),
                  (512,256,1,1,0),
                  (256,512,3,1,1),
                  ("FPN_Route_2",512,256,1,1,0), #Route_2
                  (256,128,1,1,0),
                  #Here upsampling occur..
                  #After this concatenation with the Route_1 occur...
                  (256,128,1,1,0),
                  (128,256,3,1,1),
                  (256,128,1,1,0),
                  (128,256,3,1,1),
                  (256,128,1,1,0), #it means that we can used these as an input.....
                  ["First_Detection",(128,256,3,1,1),(256,255,1,1,0), (255,255,3,2,1)], #First Detection after linear interpolation output: (1,255,52,52) #15, #17
                  (128,256,3,2,1),
                  #After this layer processing concatenation occur of FPN features with the PAN net ...(128,256,3,2,1),
                  (512,256,1,1,0),
                  (256,512,3,1,1),
                  (512,256,1,1,0),
                  (256,512,3,1,1),
                  (512,256,1,1,0),
                  ["Second_Detection",(256,512,3,1,1),(512,255,1,1,0), (255,255,3,2,1)], #Second detection..after linear interpolation output: (1,255,26,26),#24,#26
                  (256,512,3,2,1),
                  #After this layer concatentation will performed with ...(512,256,1,1,0)
                  (1024,512,1,1,0),
                  (512,1024,3,1,1),
                  (1024,512,1,1,0),
                  (512,1024,3,1,1),
                  (1024,512,1,1,0),
                  ["Third_Detection",(512,1024,3,1,1),(1024,255,1,1,0), (255,255,3,2,1)], #Third detection..after linear interpolation output: (1,255,13,13),#33,#35



]

class Conv_Operation(torch.nn.Module):
  def __init__(self,input,output, filter_size, stride, padding):
    super(Conv_Operation,self).__init__()
    self.Conv=torch.nn.Conv2d(input,output,filter_size, stride=stride, padding=padding)
    self.norm=torch.nn.BatchNorm2d(output) #yes we need batch normalization
    self.mish= torch.nn.Mish()


  def forward(self, input):
    input= self.Conv(input)
    input= self.norm(input)
    input= self.mish(input)

    return input

class Leaky_Relu(torch.nn.Module):
  def __init__(self,input,output, filter_size, stride, padding):
    super(Leaky_Relu,self).__init__()
    self.Conv=torch.nn.Conv2d(input,output,filter_size, stride=stride, padding=padding)
    self.norm=torch.nn.BatchNorm2d(output) #yes we need batch normalization
    self.Leaky = torch.nn.LeakyReLU(0.01)  # Corrected 'LeakyReLU' and the parameter


  def forward(self, input):
    input= self.Conv(input)
    input= self.norm(input)
    input= self.Leaky(input)

    return input

class Shortcut(torch.nn.Module):
  def __init__(self) -> None:
    super(Shortcut,self).__init__()

  def forward(self, x, residual):
    return x + residual

class Route_Concatentation(torch.nn.Module):
  def __init__(self,layer_1,layer_2) -> None:
    super(Route_Concatentation,self).__init__()
    self.layer_1= layer_1
    self.layer_2= layer_2

  def forward(self, x):
    layer_1=self.layer_1(x)
    layer_2=self.layer_2(x)
    concatentation= torch.cat((layer_1,layer_2), 1)

    return concatentation

class Residual_block(torch.nn.Module):
  def __init__(self,input_filters,Inner_filters, output_filters):
    super(Residual_block,self).__init__()
    self.Conv_1=torch.nn.Conv2d(in_channels=input_filters, out_channels=Inner_filters, kernel_size=(1,1), stride=1, padding=0)
    self.Conv_2=torch.nn.Conv2d(in_channels=Inner_filters, out_channels=output_filters, kernel_size=(3,3), stride=1, padding=1)
    self.norm_1=torch.nn.BatchNorm2d(Inner_filters) #yes we need batch normalization
    self.norm_2=torch.nn.BatchNorm2d(output_filters) #yes we need batch normalization
    self.Mish_2= torch.nn.Mish()
    self.Short_Cut= Shortcut()




  def forward(self, x):
    input= self.norm_1(self.Conv_1(x))
    input= self.Mish_2(self.norm_2(self.Conv_2(input)))
    output= self.Short_Cut(x,input)

    return output

def Feature_Pyramid_Pooling(x,input,output):

    Conv_1=torch.nn.Conv2d(input,output,1, stride=1, padding=0)
    norm_1=torch.nn.BatchNorm2d(output) #yes we need batch normalization

    Conv_2=torch.nn.Conv2d(input//2,output,1, stride=1, padding=0)
    norm_2=torch.nn.BatchNorm2d(output) #yes we need batch normalization
    Leaky = torch.nn.LeakyReLU(0.01)  # Corrected 'LeakyReLU' and the parameter
    output= norm_1(Conv_1(x))
    output= Leaky(norm_2(Conv_2(output)))


    return output

import torch.nn.functional as F
class CSP_Darknet_53(torch.nn.Module):
  def __init__(self,Architecture):
    super(CSP_Darknet_53,self).__init__()
    self.layers= torch.nn.ModuleList()
    self.architecture= Architecture
    for i in range(len(self.architecture)):

      if type(self.architecture[i])==tuple:
        if self.architecture[i][0]=="Route_1":
          self.Route_1=Conv_Operation(self.architecture [i][1],self.architecture[i][2],self.architecture[i][3],self.architecture[i][4],self.architecture[i][5])
          self.layers.append(self.Route_1)

        elif self.architecture[i][0]=="Route_2":
          self.Route_2=Conv_Operation(self.architecture [i][1],self.architecture[i][2],self.architecture[i][3],self.architecture[i][4],self.architecture[i][5])
          self.layers.append(self.Route_2)

        else:
          Conv=Conv_Operation(self.architecture [i][0],self.architecture[i][1],self.architecture[i][2],self.architecture[i][3],self.architecture[i][4])
          self.layers.append(Conv)


      if type(self.architecture[i])==dict:
        Route_layer= self.architecture[i]
        Route_layer= Route_layer["Skip_Connection"]
        Skip_Connection=Conv_Operation(Route_layer[0],Route_layer[1],Route_layer[2],Route_layer[3],Route_layer[4])
        self.layers.append(Skip_Connection)
        self.layers.append(Route_Concatentation(Skip_Connection,Route))




      if type(self.architecture[i])==list and self.architecture[i][0]=="Route":
        Route_layer= self.architecture[i][1]
        Route=Conv_Operation(Route_layer[0],Route_layer[1],Route_layer[2],Route_layer[3],Route_layer[4])
        self.layers.append(Route)


      elif type(self.architecture[i])==list and self.architecture[i][0]=="Residual_Block":
        for j in range(self.architecture[i][4]):
          Residual= self.architecture[i]
          Residual_Block=Residual_block(Residual[1],Residual[2],Residual[3])
          self.layers.append(Residual_Block)


      elif type(self.architecture[i])==list and self.architecture[i][0]=="Leaky":
        Leaky= self.architecture[i]
        Leaky= Leaky[1]
        print(Leaky)
        Leaky_relu_block=Leaky_Relu(Leaky[0],Leaky[1],Leaky[2],Leaky[3],Leaky[4])
        self.layers.append(Leaky_relu_block)









  def forward(self, x):
    previous_value = None
    alternate = False  # Flag to toggle between left and right processing
    process_left = True  # Flag to track if we should process the left or right branch

    for layer in self.layers:
        previous_value = x
        x = layer(x)

        # Print current layer's output
        current_value = x
        print(f"output shape: {current_value.shape}\n")


        # Capture the outputs of Route_1 and Route_2 when they are processed
        # here we can check that whether the layer is an object of Conv_operation which is class
        if hasattr(self, 'Route_1') and isinstance(layer, Conv_Operation) and layer == self.Route_1:
          self.route_1_output = x

        if hasattr(self, 'Route_2') and isinstance(layer, Conv_Operation) and layer == self.Route_2:
          self.route_2_output = x




        if (current_value.shape[2:]) == (previous_value.shape[2] // 2, previous_value.shape[3] // 2):
            if current_value.shape == (1, 64, 304, 304):
                left = x[:, :, :, :]  # For the first left block
                right = x[:, :, :, :]  # For the first right block

            else:
                left = x[:, :x.shape[1] // 2, :, :]  # First half for the left branch
                right = x[:, x.shape[1] // 2:, :, :]  # Second half for the right branch


            alternate = True  # Set flag to start alternating
            process_left= True

        if alternate:
            # Alternate between left and right for the next two layers
            if process_left:
                x = left  # Use left tensor for this layer
                process_left= False
            else:
                x = right  # Use right tensor for this layer
                alternate= False


    self.last_output = x
    return x

class Spatial_pyramid_pooling(torch.nn.Module):
  def __init__(self):
    super(Spatial_pyramid_pooling,self).__init__()
    self.max_pooling_1= torch.nn.MaxPool2d(kernel_size=(5,5),stride=1,padding=2)
    self.max_pooling_2= torch.nn.MaxPool2d(kernel_size=(9,9),stride=1,padding=4)
    self.max_pooling_3= torch.nn.MaxPool2d(kernel_size=(13,13),stride=1,padding=6) #Updation performed...



  def forward(self, x):
    output_1= self.max_pooling_1(x)
    output_2= self.max_pooling_2(x)
    output_3= self.max_pooling_3(x)
    output= torch.cat((output_1,output_2,output_3),1)

    return output

class CSP_Darknet(torch.nn.Module):
  def __init__(self):
    super(CSP_Darknet,self).__init__()
    self.Architecture=Architecture
    self.FPN_Architecture= FPN_Architecture
    self.CSP_Darknet_53= CSP_Darknet_53(self.Architecture)
    self.SPP= Spatial_pyramid_pooling()
    self.FPN_Implementation= self.FPN_Architecture_Implementation()




  def FPN_Architecture_Implementation(self):
    self.layers= torch.nn.ModuleList()
    for i in range(len(self.FPN_Architecture)):


      if type(self.FPN_Architecture[i])==tuple:
        if self.FPN_Architecture[i][0]=="FPN_Route_1":
          self.FPN_Route_1=Leaky_Relu(self.FPN_Architecture[i][1],self.FPN_Architecture[i][2],self.FPN_Architecture[i][3],self.FPN_Architecture[i][4],self.FPN_Architecture[i][5])
          self.layers.append(self.FPN_Route_1)

        elif self.FPN_Architecture[i][0]=="FPN_Route_2":
          self.FPN_Route_2=Leaky_Relu(self.FPN_Architecture[i][1],self.FPN_Architecture[i][2],self.FPN_Architecture[i][3],self.FPN_Architecture[i][4],self.FPN_Architecture[i][5])
          self.layers.append(self.FPN_Route_2)

        else:
          Conv=Leaky_Relu(self.FPN_Architecture[i][0],self.FPN_Architecture[i][1],self.FPN_Architecture[i][2],self.FPN_Architecture[i][3],self.FPN_Architecture[i][4])
          self.layers.append(Conv)



      if type(self.FPN_Architecture[i])==list and self.FPN_Architecture[i][0]=="Leaky":
        Leaky= self.FPN_Architecture[i]
        Leaky= Leaky[1]
        print(Leaky)
        Leaky_relu_block=Leaky_Relu(Leaky[0],Leaky[1],Leaky[2],Leaky[3],Leaky[4])
        self.layers.append(Leaky_relu_block)

      if type(self.FPN_Architecture[i])==list and (self.FPN_Architecture[i][0]=="First_Detection" or self.FPN_Architecture[i][0]=="Second_Detection" or self.FPN_Architecture[i][0]=="Third_Detection"):
        for j in range(len(self.FPN_Architecture[i])-1):
          Leaky= self.FPN_Architecture[i][j+1]
          Leaky= Leaky
          print(Leaky)
          Leaky_relu_block=Leaky_Relu(Leaky[0],Leaky[1],Leaky[2],Leaky[3],Leaky[4])
          self.layers.append(Leaky_relu_block)

    return self.layers





  def forward(self, x):
    output= self.CSP_Darknet_53(x)
    last_output = self.CSP_Darknet_53.last_output
    spp_output = self.SPP(last_output)  # Apply SPP on the last output

    final_output = torch.cat((spp_output, last_output), 1)

    for layer_index, layer in enumerate(self.FPN_Implementation):
      Alternate= False
      final_output= layer(final_output)
      print("output shape with Leaky_Relu:",final_output.shape)




      if hasattr(self, 'FPN_Route_1') and isinstance(layer, Leaky_Relu) and layer == self.FPN_Route_1:
        self.route_1_output_FPN = final_output

      if hasattr(self, 'FPN_Route_2') and isinstance(layer, Leaky_Relu) and layer == self.FPN_Route_2:
        self.route_2_output_FPN = final_output






      if (final_output.shape[1], layer_index) in [(256, 3), (128, 9)]:
        if layer_index==3:
          final_output=  F.interpolate(final_output, scale_factor=2, mode='bilinear', align_corners=False) #up sampling...
          print(f"Upsampling:{final_output.shape}\n")
        elif layer_index==9:
          final_output=  F.interpolate(final_output, scale_factor=2, mode='bilinear', align_corners=False) #up sampling...
          print(f"Upsampling:{final_output.shape}\n")

        Alternate= True


      elif (final_output.shape[1], layer_index) in [(128, 14),(255,17),(256,18),(256,23),(255,26),(512,27),(255,35)]:

        if (layer_index == 14 or layer_index==23):
          Linear_interp = final_output


        elif (layer_index ==18 or layer_index==27):
          if layer_index==18:
            final_output= torch.cat((final_output,self.route_2_output_FPN),1)
            print(f"Output after Concatentation of FPN and PANnet input:{final_output.shape}")
          else:
            final_output= torch.cat((final_output,self.route_1_output_FPN),1)
            print(f"Output after Concatentation of FPN with PANnet Input:{final_output.shape}")


        else:
          if layer_index==17:
            final_output = F.interpolate(final_output, size=(52, 52), mode='bilinear', align_corners=False)
            print(f"First Detection:{final_output.shape}\n\n")
            final_output= Linear_interp


          elif layer_index==26:
            final_output = F.interpolate(final_output, size=(26, 26), mode='bilinear', align_corners=False)
            print(f"Second Detection:{final_output.shape}\n\n")
            final_output= Linear_interp


          elif layer_index==35:
            final_output = F.interpolate(final_output, size=(13, 13), mode='bilinear', align_corners=False)
            print(f"Final Detection:{final_output.shape}\n\n")



      if Alternate== True:
        #The we can concatenate the backbone layers with FPN
        Up_Scale_feature= final_output
        if layer_index==3:
          final_output= Feature_Pyramid_Pooling(self.CSP_Darknet_53.route_2_output,512,256)
          print("Output Shape:",final_output.shape)
        else:
          final_output= Feature_Pyramid_Pooling(self.CSP_Darknet_53.route_1_output,256,128)
          print("Ouput shape:",final_output.shape)


        final_output= torch.cat((Up_Scale_feature,final_output),1) #Route_2 concatenation with FPN upsample features....
        #Here Concatenation performed ....
        print("Output shape after concatenation:",final_output.shape)


    return final_output

#so here it work properly
if __name__== "__main__":
    model = CSP_Darknet()
    # Sample input tensor with batch size of 1 and image size 416x416
    x = torch.randn(1, 3, 608, 608)
    output_x = model(x)
    print("Output shape of x with the input shape is:(1, 3, 608, 608)",output_x.shape)

